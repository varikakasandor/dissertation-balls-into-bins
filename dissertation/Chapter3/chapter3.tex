%!TEX root = ../thesis.tex
\chapter{Implementation}\label{implementation}

\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi


\section{Two-Thinning}


\subsection{Deep Q-Learning Implementation}


Unlike for many supervised learning problems, there is no library for RL that provides one-liner solutions to my settings. While there are some libraries for RL (e.g. Stable Baselines, Pyqlearning), they are usually very hard to customize, and for example the format of the state space is very unique RL problems. Also, implementing the basic RL logic is not too long, so I made the decision to implement the RL logic myself, and use Pytorch for the NNs.


\NOTE{A}{At this point I expect the reader to understand Deep Q-Learning in enough detail, so that I just have to note some implementation details.}


\subsubsection{Deep Q-Network (DQN)} \label{DQN}


The NN used in Deep Q-Learning is often referred to as Deep Q-Network (DQN). 


There are two ways to formulate the \textsc{Two-Thinning} setting as a MDP. One is to treat (load vector, primary bin) pairs as states, and accept/reject as actions. The other is to treat load vectors as states, and thresholds for accepting/rejecting as actions. The latter implicitly makes use of a very important (also intuitive) property of an optimal policy:


\begin{lemma} \label{lemma: thresholdproperty}
At a load vector $v$, the optimal policy doesn't reject a primary bin with load $x$, if it would accept a primary bin with load $y>x$. That is, the optimal strategy is always a threshold strategy, that rejects above a load value $z$ and accepts otherwise.
\end{lemma}


\NOTE{A}{Add proof, or say something about it.}

Since this lemma holds, I chose the second formulation, which using this lemma, restricts the space in which the algorithm has to search for the optimal policy. We will see later that this lemma is useful in optimising the dynamic programming approach as well.


Therefore the input to the DQN is a load vector $v$, and the output is an estimate of $Q(v, a)$ for each possible thresholds. There is no point in using non-integer thresholds, since the load of the primary bin is always a non-negative integer. Also, there is no benefit in using a threshold larger than the current maximum load, as setting the threshold to that instead achieves the same (accepting everything). Similarly, as a consequence of Lemma \ref{lemma: thresholdproperty} \NOTE{A}{Is it a corollary strictly speaking?}, the best possible load primary bin load is $0$, therefore it never makes sense to reject it (or in general reject a bin with minimum load). So theoretically, the possible thresholds should be from $0$ to $m$, that is, the output of the DQN should have size $m+1$. Even though it is possible that a load of $m$ is reached during execution, it has a very small probability (more details in Chapter \ref{evaluation}), so I decided to restrict the maximum threshold available for the algorithm. The optimal maximum threshold should depend on $n$ and $m$, and together with other hyperparameters, I provide their final values and their importance in later sections. 


Now that the output of the DQN is fixed, the other question is how to represent the input to the DQN, which is a load vector $v$. First of all, I always sort the load vector before feeding it into the network. This is possible because the order of the bins doesn't matter in \textsc{Two-Thinning}, unlike in \textsc{Graphical Two-Choice}. This sorting makes the function that the network has to learn easier (it doesn't have to learn permutation invariance), and it will be important when using RNN as the architecture. Then, I one-hot encode each load value independently. The motivation for this is that NNs often can't learn well with ordinal data - they would think that the load values are on a continuous scale, but in fact, the real difference between e.g. load $0$ and $1$ is much smaller than the real difference between load e.g. $m-1$ and $m$. \NOTE{A}{https://stats.stackexchange.com/questions/423820/do-ordinal-variables-require-one-hot-encoding}. The natural range of one-hot encoding for the load values is from $0$ to $m$, but to condense the input to the network (and hence reduces the number of weights to train) I tried collapsing the very high load values with very small probability, into one ``outlier'' class. This didn't provide better results, so I do not discuss it any further.


Now that the input and output format is laid down, I turn to the discussion of the architecture of the DQN. From the load values in a load vector, the highest ones are much more important, since we care about minimising the maximum load. On the extreme, if the bin with the maximum load had one more ball in it, that would be more significant than if an average bin had one more ball in it. Hence, my idea was to process the load vector in increasing order of the loads, that is, in increasing order of importance. For this, using an RNN is a perfect choice. It is used for processing sequential input, and what is more, by the end it forgets the earlier inputs to some extent, and therefore focuses more on the final (in our case most important) inputs, as desired. As mentioned in Chapter \ref{preparation}, there are more complex versions of RNNs, whose main advantage is that they forget less. However, as this forgetting property is desirable in our case, this advantage is not really an advantage. On the other hand, they are less susceptible to the vanishing/exploding gradient problems \cite{noh2021rnnvanishinggradient}, so they are sometimes easier to train, therefore I tried GRU and LSTM as well, but they didn't provide additional benefits, so I will stick to the vanilla RNN. After the last input, we can extract the hidden state of the RNN, whose exact size I will discuss in the hyperparameters section. To get the output, the estimated $q(v,a)$ values for each possible threshold $a$, I apply some (as for other hyperparameters, I discuss the exact values later) fully convolutional (linear) layers on the final hidden state of the RNN. These linear layers not only bring the output to the correct shape, they also transform from the internal representation of the RNN to the space of the action-values. \NOTE{A}{Explain some more?}\NOTE{A}{Add a figure about the DQN architecture.}


\subsubsection{Stabilising Training}


In practice, Deep Q-Learning - and in general any off-policy deep reinforcement learning algorithm - can become unstable during training. There are several methods proposed to address this issue, out of which the three most popular are the following. I added these to my implementation as well.


\paragraph{Experience Replay}


Experience replay was first introduced in \cite{lin1992experiencereplay}. The problem it is trying to solve is that in vanilla Deep Q-Learning, subsequent the update steps are correlated - i.e. in our case, after we update network around a load vector with $x$ balls, we next update it around a load vector with $x+1$ balls. This correlation stems from updating in the same order as executing the actions in the game. In order to get the theoretical guarantees for these supervised learning-like updates, we need to have independent identically distributed, so certainly not correlated training samples. The idea in experience replay, is to instead of calling the update rule \ref{eq:deep-q-learning-update-with-semi-gradient} on the current $(s, a, s', r)$ tuple, we store this tuple in an experience replay buffer. Then, after every fixed number of steps, a batch of tuples are samples uniformly at random from the buffer, and the DQN is updated according to those tuples. Another benefit of the idea is that tuples can be reused multiple times, leading to a more efficient learning. Nevertheless, there is a size limit on the buffer to get rid of outdated samples, so whenever it is full, a tuple is deleted from it. I implemented it using a deque data structure. Finally, note the importance of sampling a batch of tuples when updating, and not only a single tuple - it has been shown that batching helps stabilising learning \cite{qian2020batchingsgd}.


\paragraph{Target Network}


The idea of a target network was first introduced in \cite{argueta1992targetnetwork}. The main difference between Q-Learning and Deep Q-Learning is that in the former, during one step we only update a single value in the Q-table, while in the latter, due to the nature of NNs, many neighbouring values are also updated (it is a continuous function approximation). This becomes problematic, since looking at the update rule \ref{eq:deep-q-learning-update-with-semi-gradient} for Deep Q-Learning, to update at a Q-value $Q_{\mathbf{w_t}}(s_t, a_t)$, a neighbouring Q-value $Q_{\mathbf{w_t}}(s_{t+1}, a')$ is used. This can lead to a blow-up in the Q-values, intuitively, as a chain reaction. \NOTE{A}{Honestly, I don't understand it at all... I couldn't find any good explanation of this.}. The idea with target networks, is to have two copies of the same network architecture: one whose weights are updated by the update rule, and one that provides $Q_{\mathbf{w_t}}(s_{t+1}, a')$ of the ``target'' part $r_{t+1}+ \max_{a'} Q_{\mathbf{w'_t}}(s_{t+1}, a')$ of the rule. This weights $w'$ of the target network are updated to that of the main network periodically, every fixed number of training episodes.


\paragraph{Using a better Optimiser}


Looking more closely at the update rule \ref{eq:deep-q-learning-update-with-semi-gradient}, and using the batching technique outlined for experience replay, we can see that this resembles the update rule of the Stochastic Gradient Descent (SGD) optimiser method. Therefore, instead of SGD, we could use any other optimiser method. In practice, the most widely used optimiser for Deep Q-Learning is the ADAM optimiser \cite{kingma2015adamoptimiser}. Briefly, it is a more complex version of SGD that doesn't use the raw gradients, and instead keeps track of moving averages and variances of the gradients, to adjust the step size in the size of the current gradient. An important parameter of ADAM is its learning rate, which I will optimise using hyperparameter optimisation. Also, a widely used extension in deep reinforcement learning is gradient clipping, which doesn't allow absolute gradient values to go above $1$, and has been shown to speed up training \cite{zhang2020gradientclipping}. I added gradient clipping to my implementation.\\



There are plenty of more ideas in the literature to improve the training of DQN, for example the so-called double learning \cite{hasselt2010doubleqlearning}, which argues that vanilla (deep) Q-Learning overestimates the action values, since under the hood it uses $\mathbb{E}[\max_a(Q(s,a)]$ and not $\max_a(\mathbb{E}[Q(s,a)])$. I do not consider these possibilities any further.


\subsection{Deep Sarsa-Learning Implementation}


\NOTE{A}{Maybe not needed? I don't have too many words left to use. Also, this is nothing special really.}


\subsection{Ideas Implemented for Improvement} \label{improvementideas}


In this section I outline ideas independent of DQN that I used to improve the performance of my RL algorithm. Some of these are well-known general ideas in RL, while others are specific to the \textsc{Two-Thinning} Balls into Bins setting. I will highlight why each of the ideas were worth considering for this setting.


\subsubsection{Reward Shaping}


As discussed in Chapter \ref{preparation}, the most direct way to formulate the \textsc{Two-Thinning} setting as a MDP is to only give reward after a final state, i.e. when all the balls have been placed. The problem with this is that rewards are very sparse - concretely, the agent only receives one reward per execution. It is problematic because until the final rewards propagate back to earlier states (that is, load vectors with few balls), the updates at earlier states are not justified - they use a zero reward and a randomly initialised next state to update the current state. Hence, the idea is to inject additional rewards into the MDP, while maintaining correctness: the optimal policy for the new modified MDP has to be optimal for the original one as well, and hence for \textsc{Two-Thinning} too. There is a very neat results proved in \cite{ng1999rewardshaping} about exactly what extra rewards can be used while maintaining the correctness property: the extra reward, when moving from state $s$ to state $s'$ using action $a$ has to be in the form $\Phi(s')-\Phi(s)$, i.e. the difference of the so-called potential functions of the two states. Intuitively, the potential function should denote an estimate of how good a state is with respect to the original (in our case final) rewards. I experimented with several candidate potential functions, and here I present the most promising ones, while I compare them numerically in the hyperparameter optimisation section \NOTE{A}{I write this sentence too many times...}.


\begin{itemize}
    \item
    $\Phi(v)=-maxload(v)$. This is a natural extension of the final reward, which is also the same expression. In practice, it turns into getting a reward $-1$ if the current ball has been allocated such that the max load has increased, and otherwise still no reward.
    \item
    $\Phi(v)=-std(v)$, where $std(v)$ is the standard deviation of the load vector. This is an extension of the previous idea, which differentiates between cases when the max load hasn't increased - it is much better if the ball is allocated in the most underloaded bin than if in the second most overloaded.
    \item
    $\Phi(v)=\sum_{x\in v} e^{\alpha * (x - sum(v) / n)}$, where $\alpha$ is a hyperparameter. The motivation for this is similar to the previous one. This function is called the exponential potential function, and it has first been introduced as a useful tool in proving bounds for Balls into Bins settings \cite{ghosh1999exponentialpotential} \NOTE{A}{I feel that I have way too many entries in the bibliography overall, and I am not reusing them efficiently.}. As a relevant example, if the exponential potential of a load vector $v$ is $O(len(v))$, it follows that $maxload(v) < (avg(v)+log(len(v)))$.
\end{itemize}


\subsubsection{Curriculum Learning}


\subsubsection{Normalised Load}


\subsection{Dynamic Programming}


On the other hand, for smaller instances we can use the dynamic programming based on the Bellman equations (and the known environment) to get the exact optimal policy, and we will get back to this in the next chapter.

\subsection{Other Strategies}


\subsection{Restricting Available Information} \label{lesssharedstate}


\section{K-Thinning}


\subsection{Framing it as a MDP}


\subsection{Dynamic Programming}


\subsection{Other Strategies}


\section{Graphical Two-Choice}


\subsection{Framing it as a MDP}


\subsection{Graph Structures}


\subsection{Dynamic Programming}


\subsection{Other Strategies}


\section{Hyperparameter Optimisation}


\subsection{Genetic Algorithm}


\subsection{Weights and Biases tool}


\section{Repository Overview}


\subsection{Extensibility}
\NOTE{A}{Software Engineering Practices. Extensibility is too focused, it should be more than that.}

\subsection{Unit Tests}

