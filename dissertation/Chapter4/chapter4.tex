%!TEX root = ../thesis.tex
\chapter{Evaluation}\label{evaluation}


\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi


In this chapter I provide quantitative and qualitative analysis and comparison of the protocols presented in previous chapters, highlighting the properties of the optimal \DP strategy for \TwoThinning, the impact of $k$ on \KThinning, the limitations of the \Greedy Strategy for \GraphicalTwoChoice, and the patterns in \DQL training. I start by reviewing the success criteria.


\section{Success Criteria Revisited}

I successfully completed all the success criteria:

\begin{itemize}
    \item 
    I implemented \DQL RL algorithms for three protocols (\S\ref{dqn-implmentation-two-thinning}, \S\ref{dqn-implementation-k-thinning}, \S\ref{dqn-implmentation-graphical-two-choice}) and I analyse their performance throughout this chapter.
    \item
    I implemented DP algorithms (\S\ref{two-thinning-dp}, \S\ref{k-thinning-dp}, \S\ref{graphical-two-choice-dp}) and heuristic strategies (\S\ref{two-thinning-otherstrategies}, \S\ref{k-thinning-otherstrategies}, \S\ref{graphical-two-choice-otherstrategies}) for three protocols.
    \item
    I thoroughly compared the strategies (\S\ref{two-thinning-comparison}, \S\ref{k-thinning-comparison}, \S\ref{graphical-two-choice-comparison}) and evaluated the training performance and the hyperparameters of RL (\S\ref{dql-analysis-two-thinning}, \S\ref{dql-analysis-k-thinning}, \S\ref{dql-analysis-graphical-two-choice}).
    \item
    I implemented a flexible object-oriented balls-into-bins environment (\S\ref{repository-overview}) and used it for the comparisons and the various analyses in this chapter.
\end{itemize}


I also accomplished two extensions:



\begin{itemize}
    \item 
    I analysed the behaviour of the strategy learnt by RL, comparing it to other strategies (\S\ref{dql-analysis-two-thinning}, \S\ref{dql-analysis-graphical-two-choice}).
    \item
    I formulated some conjectures (\S\ref{conjecture: two-thinning-increasing-threshold}, \S\ref{conjecture: two-thinning-smooth-threshold}) and formally proved several lemmas (\S\ref{lemma: everystatereachable}, \S\ref{lemma: thresholdproperty}, \S\ref{lemma: k-thinning-monotone}, \S\ref{lemma: greedy-suboptimal}) about optimal strategies.
\end{itemize}


\section{General Notes} \label{evaluationnotes}

I compared the strategies across \NumberofRuns runs, calculating the average score (final maximum load) of the runs and the approximate $95\%$ confidence intervals for the estimated mean score, based on the Central Limit Theorem. For showing that one strategy is significantly better than the other, I calculated the $p$-values of one-tailed Welch's t-tests~\cite{welch1947ttest}, which work for unknown variances, and due to the large number of samples, its normality assumption is also reasonable for the mean score. For higher confidence I conducted the non-parametric Wilcoxon signed-rank tests~\cite{wilcoxon1992test} as well and the results were confirmed.


For space reasons I usually chose some representative values of $n$ and $m$ to illustrate the general trends, covering different ranges of $n$ and different average final loads $\frac{m}{n}$.


I utilised parallel processing to speed up the execution of \DQL policies, which are the bottleneck for thorough and statistically significant evaluation. To exploit the parallelism of the GPU even with the inherently sequential MDP, I conducted several (usually $64$) independent runs in parallel. Note that this is much simpler for \TwoThinning and \GraphicalTwoChoice, where the number of steps in the MDP is fixed ($m$) in any execution.


Even with this evaluation speedup, the usability of \DQL for large values of $n$ and $m$ is still limited due to the training time. The largest range for which I successfully trained a RL algorithm in at most a day is $n=1000$, $m=1500$, but I decided to focus on slightly smaller values for the evaluation as that is more illustrative and complementary to the literature available.  


I note that there is merit in not just executing several runs with the trained \DQL model, but also retraining it several times (e.g.\ $20$ runs with each of $5$ independently trained model), because the rewards received during training are also probabilistic. This idea would rather evaluate the robustness of the \DQL framework for balls-into-bins protocols, but I decided to stick to the more classical single trained model and essentially evaluate the peak performance.


Even though the \DP strategies are optimal by construction, they are too slow for larger values of $n$ and $m$ which I denote by Time Limit Exceeded (TLE). The limit was set to $12$ hours of training/hyperparameter tuning/precomputation overall for all the strategies. Also, even though we have the exact expected final maximum load of the \DP strategy, I decided to run it just like the other strategies for fair comparison (e.g.\ large outliers that the \DP strategy takes into account occur very rarely during execution), so it might not always be the best in the comparisons on \NumberofRuns runs.


For the \Threshold strategies, I calculated the optimal value of $l$ by modelling it as a Multi-Armed Bandit problem (covered in II Randomised Algorithms)~\cite{katehakis1987multiarmedbandit}, using an $\epsilon$-greedy technique with $\epsilon=0.1$ (details omitted).




\section{\TwoThinning}


\subsection{Comparison of Strategies}\label{two-thinning-comparison}

Now I present a comparison of the \TwoThinning strategies.


\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
                                & \multicolumn{3}{c|}{$n=5$} & \multicolumn{3}{c|}{$n=20$} & \multicolumn{3}{c|}{$n=50$} \\ \hline
Strategy                                & $m=5$ & $m=10$ & $m=25$ & $m=20$ & $m=60$ & $m=400$ & $m=50$ & $m=200$ & $m=2500$ \\ \hline
\AlwaysAccept & 2.30 $\pm$ 0.05 & 3.74 $\pm$ 0.07 & 7.74 $\pm$ 0.11 & 3.18 $\pm$ 0.06 & 6.59 $\pm$ 0.09 & 28.72 $\pm$ 0.20 & 3.78 $\pm$ 0.07 & 9.14 $\pm$ 0.10 & 66.58 $\pm$ 0.31 \\ \hline
\LocalRewardOptimiser & 1.84 $\pm$ 0.04 & 3.00 $\pm$ 0.04 & \textbf{6.07 $\pm$ 0.05} & 2.26 $\pm$ 0.04 & 4.70 $\pm$ 0.05 & 22.46 $\pm$ 0.06 & \textbf{2.59 $\pm$ 0.05} & \textbf{6.32 $\pm$ 0.04} & 53.95 $\pm$ 0.07 \\ \hline
\MeanThinning & 1.87 $\pm$ 0.04 & 3.04 $\pm$ 0.05 & 6.22 $\pm$ 0.06 & 2.53 $\pm$ 0.05 & 5.02 $\pm$ 0.07 & 22.60 $\pm$ 0.09 & 3.02 $\pm$ 0.05 & 6.95 $\pm$ 0.07 & 53.39 $\pm$ 0.10 \\ \hline
\DP & \textbf{1.81 $\pm$ 0.04} & \textbf{2.99 $\pm$ 0.04} & 6.12 $\pm$ 0.05 & \textbf{2.21 $\pm$ 0.04} & \textbf{4.59 $\pm$ 0.05} & TLE & 2.59 $\pm$ 0.05 & TLE & TLE \\ \hline
\DQN & 1.89 $\pm$ 0.04 & 3.02 $\pm$ 0.05 & 6.23 $\pm$ 0.06 & 2.39 $\pm$ 0.05 & 4.84 $\pm$ 0.07 & \textbf{22.18 $\pm$ 0.06} & 2.84 $\pm$ 0.05 & 6.84 $\pm$ 0.07 & \textbf{53.26 $\pm$ 0.09} \\ \hline \Threshold & 2.25 $\pm$ 0.05 & 3.41 $\pm$ 0.06 & 6.64 $\pm$ 0.07 & 2.60 $\pm$ 0.05 & 5.17 $\pm$ 0.06 & 24.18 $\pm$ 0.10 & 3.01 $\pm$ 0.05 & 7.07 $\pm$ 0.07 & 56.52 $\pm$ 0.12 \\ \hline 
\end{tabular}}
\caption{Average maximum load of \TwoThinning strategies with $95\%$ confidence intervals.}
\label{tab:two-thinning-comparison}
\end{table}



The key conclusions from Table~\ref{tab:two-thinning-comparison} are:

\begin{itemize}
    \item The \LocalRewardOptimiser strategy is competitive with the \DP strategy for medium values of $n$ and $m$, even though it only tries avoiding currently maximum loaded bins. For large $\frac{m}{n}$ ratios however, the difference between two non-maximum loaded bins becomes more significant, so the strategy performs worse compared to others.
    \item The \DQN strategy is significantly better than the \MeanThinning and \Threshold strategies for all $n\leq 20$, under a largest $p$-value of $0.03$ and $1.08\cdot 10^{-6}$ respectively.
    \item The \DQN strategy is consistent across different values of $n$ and $m$, but it can outperform all the other strategies only for large $\frac{m}{n}$ ratios. Note that arguably the $m\gg n$ case is the most directly applicable to real-world scenarios, so the performance of \DQL in that case is a good sign for future work.
\end{itemize}


\subsection{Analysis of Optimal Strategies}

Analysing the optimal \DP strategy for \TwoThinning, I formulated the following conjecture.

\begin{conjecture}\label{conjecture: two-thinning-increasing-threshold}
There exists an optimal (slicing) strategy such that its chosen thresholds are non-decreasing during an execution. That is, if for the $i$-th ball the protocol chooses a threshold $x$, then for every later ball of the same execution, a threshold $y\geq x$ is chosen (as observed in the experiments, see Figure~\ref{dp-increasing-threshold}).
\end{conjecture}


\begin{figure}
\centering
\begin{minipage}[t]{.32\linewidth}
  \centering
  \includegraphics[scale=0.36]{Chapter4/Figs/dp_increasing_threshold_1.pdf}
\end{minipage}\hfill
\begin{minipage}[t]{.32\linewidth}
  \centering
  \includegraphics[scale=0.36]{Chapter4/Figs/dp_increasing_threshold_2.pdf}
\end{minipage}\hfill
\begin{minipage}[t]{.32\linewidth}
  \centering
  \includegraphics[scale=0.36]{Chapter4/Figs/dp_increasing_threshold_3.pdf}
\end{minipage}
\caption{Runs of the \DP strategy for $n=5$, $m=25$ showing the increasing threshold property. Note that the actual load vectors are not displayed.}
\label{dp-increasing-threshold}
\end{figure}


This is a surprising conjecture, because the \Threshold strategy and other \textit{asymptotically} optimal strategies for related problems do not have this property, they are mostly not even slicing strategies~\cite{feldheim2021longtermthinning}.


\begin{remark}
Conjecture~\ref{conjecture: two-thinning-increasing-threshold} has been verified by showing that this property holds for the (optimal) \DP strategy for several combinations of $n$ and $m$. 
\end{remark}


To get a deeper understanding of the optimal (\DP) strategy, using an auxiliary DP algorithm, I analysed the probabilities of it reaching different states (sorted load vectors) during an execution (see Figure~\ref{two-thinning-dp-state-distribution}). The main conclusions are:

\begin{itemize}
    \item There is a big gap between the probabilities of reaching different states by the optimal strategy. For example, the most likely final state $(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2)$ has a probability of almost a quarter, which is very large among $627$ possible final states.
    \item The entropy of the probabilities of the final states is $\approx 3.36$ bits, which is nearly a third of the entropy of a uniform distribution with the same number of states.
    \item In future work, the presence of many small probability states could be exploited to optimise the training of RL algorithms, e.g.\ finding the most relevant curriculum for curriculum learning.
    \item The naive simulation of any strategy uses $m\cdot \log_2 n=20\cdot \log_2 20\approx 86$ bits for $n=m=20$, so the $86-3.36\approx 83$ bits gap highlights that naive simulation is very inefficient.
\end{itemize}


In many real-world applications it does not suffice if the maximum load is low in expectation, it should also be small with high probability. As demonstrated in Figure~\ref{two-thinning-dp-maxload-distribution}, the \DP strategy has this property (agreeing with most of the strategies/protocols in the balls-into-bins literature).

\begin{figure}
\centering
\begin{minipage}[t]{.48\linewidth}
  \centering
  \includegraphics[scale=0.5]{Chapter4/Figs/state_distribution_20_20_all_log_count.pdf}
  \caption{The distribution of the probabilities of all the states for $n=m=20$, using the \DP strategy. Due to the skewness of the distribution, the values are shown on a log-scale.}
  \label{two-thinning-dp-state-distribution}
\end{minipage}\hfill
\begin{minipage}[t]{.48\linewidth}
  \centering
  \includegraphics[scale=0.5]{Chapter4/Figs/max_load_distribution_20_20.pdf}
  \caption{The distribution of the final maximum loads for $n=m=20$, using the \DP strategy.}
  \label{two-thinning-dp-maxload-distribution}
\end{minipage}
\end{figure}


\subsection{\DQL Analysis} \label{dql-analysis-two-thinning}

One of the main difficulties for RL is that the impact of a single ball is very subtle and in general, making a small number of bad decisions might not even impact the final maximum load, so strongly negative or strongly positive rewards are not available to the agent.

\subsubsection*{Hyperparameter Analysis}


The Weights and Biases tool~\cite{biewald2020wandb} provides hyperparameter importance analysis. It calculates the correlation coefficient between the hyperparameters and the score, for each hyperparameter. To capture interactions between hyperparameters and also non-linear relationships, it additionally provides an importance score which is calculated based on Random Forests~\cite{biewald2020wandb}. Table~\ref{two-thinning-hyperparameter-importance} -- which is based on the tool -- shows the top $5$ hyperparameters for \TwoThinning based on the importance score.


We can observe that for \TwoThinning the most important hyperparameter is to use the normalised domain, which was discussed in Section~\ref{normalised-domain}. Also, limiting the maximum threshold that the agent can use efficiently restricts the search space, as discussed in Section~\ref{dqn-implmentation-two-thinning}. We can see in Appendix~\ref{hyperparameters} that the ideal maximum threshold is in fact around the target expected maximum load we want to achieve (which can be estimated in general based on theoretical results).



\newcommand{\Progress}[2]{
\begin{tikzpicture}
\draw[fill=#2!10!white] (0,0) rectangle (5, 0.3);
\draw[fill=#2!50!white] (0,0) rectangle (5 * #1, 0.3);
\end{tikzpicture}
}

\begin{table}[h]
\begin{center}
\begin{tabular}{lcc}
 \textbf{Hyperparameter} & \textbf{Importance} & \textbf{Correlation} \\
 \addlinespace[0.2cm]
 \texttt{use\_normalised} & \Progress{0.362}{blue} & \Progress{0.602}{green} \\
 \texttt{max\_threshold} & \Progress{0.141}{blue} & \Progress{0.496}{red} \\
 \texttt{num\_rnn\_layers} & \Progress{0.07}{blue} & \Progress{0.239}{green} \\
 \texttt{rnn\_hidden\_size} & \Progress{0.069}{blue} & \Progress{0.166}{green} \\
 \texttt{pre\_train\_episodes} & \Progress{0.06}{blue} & \Progress{0.103}{red} \\
\end{tabular}
\caption{\TwoThinning hyperparameter importance~\cite{biewald2020wandb} for $n=20$, $m=400$. Green indicates positive, red indicates negative correlation.}
\label{two-thinning-hyperparameter-importance}
\end{center}
\end{table}


\subsubsection*{Training}


\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{Chapter4/Figs/training_progression_20_400.pdf}
    \caption{\TwoThinning training curve for $n=20$, $m=400$.}
     \label{two-thinning-training-curve}
\end{figure}


As shown in Figure~\ref{two-thinning-training-curve}, improvement during training decays very quickly\footnote{The rest of the episodes are not shown as no more improvement has happened.}, and the agent is close to its best already at the start. These are due to the normalised load domain trick outlined in Section~\ref{normalised-domain}. The oscillations in the training curve are mostly due to the inherent randomness in the protocol.


Motivated by the fact that in the normalised domain, a constant $0$ threshold is the \MeanThinning strategy, in Figure~\ref{two-thinning-constant-offset} I tried to see if this simple \MeanThinning strategy could be improved by choosing another constant ``offset'', not $0$. For $n=50$, $m=2500$, the best offset is $1$, and it achieves on average a final maximum load of $52.5$, which is very close to the the theoretical minimum $\frac{m}{n}=50$, and also beats the \DQN strategy. It holds for general $n$ and $m$ that the best offset is usually slightly above $0$. We can also observe in Figure~\ref{two-thinning-constant-offset} that choosing a too small offset is worse than choosing a too large offset, but both sides converge to \OneChoice in the extreme.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{Chapter4/Figs/offset_analysis_50_2500.pdf}
    \caption{Comparing \ConstantOffset strategies for $n=50$, $m=2500$.}
    \label{two-thinning-constant-offset}
\end{figure}



The Deep-Q Learning algorithm -- which is as shown above, statistically significantly better than Mean-Thinning -- in fact often learns a strategy similar to a \ConstantOffset strategy, as can be seen in Figure \ref{two-thinning-dqn-thresholds}. The short sporadic ``jumps'' are very challenging for the agent to get rid of, due to the reasons outlined in Section~\ref{two-thinning-comparison}. To avoid this, future work could constrain the agent to change (and only increase) the threshold by at most $1$ after each ball, which is motivated by Conjecture~\ref{conjecture: two-thinning-smooth-threshold}.


\begin{conjecture}\label{conjecture: two-thinning-smooth-threshold}
There exists an optimal (slicing) strategy such that its chosen thresholds are changed by at most $1$ in each step during an execution.
\end{conjecture}


\begin{remark}
Together with Conjecture~\ref{conjecture: two-thinning-increasing-threshold} these would imply an optimal slicing strategy whose threshold either stays the same or increases by $1$ in each step.
\end{remark}


\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{Chapter4/Figs/dqn_learnt_thresholds_20_400.pdf}
    \caption{Analysis of the chosen thresholds of the \DQN strategy for a run of $n=20$, $m=400$.}
    \label{two-thinning-dqn-thresholds}
\end{figure}



\section{\KThinning}



\subsection{Comparison of Strategies} \label{k-thinning-comparison}


Since the drawback (TLE) of the \DP strategy has already been highlighted for \TwoThinning, I decided to focus on smaller cases here where a full comparison is feasible and highlight other patterns related to the choice of $k$.


\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
                                & \multicolumn{4}{c|}{$n=5$} & \multicolumn{4}{c|}{$n=25$}\\ \hline
                                & \multicolumn{4}{c|}{$m=20$} & \multicolumn{4}{c|}{$m=50$}\\ \hline
Strategy                                & $k=2$ & $k=3$ & $k=5$ & $k=10$ & $k=2$ & $k=3$ & $k=5$ & $k=10$ \\ \hline
\AlwaysAccept & 7.73 $\pm$ 0.11 & 7.76 $\pm$ 0.11 & 7.59 $\pm$ 0.11 & 7.67 $\pm$ 0.11 & 5.82 $\pm$ 0.09 & 5.79 $\pm$ 0.08 & 5.83 $\pm$ 0.09 & 5.86 $\pm$ 0.09 \\ \hline \LocalRewardOptimiser & \textbf{6.09 $\pm$ 0.05} & 5.74 $\pm$ 0.04 & 5.41 $\pm$ 0.04 & 5.13 $\pm$ 0.03 & 4.17 $\pm$ 0.04 & 3.73 $\pm$ 0.04 & 3.20 $\pm$ 0.04 & 3.00 $\pm$ 0.01 \\ \hline \Quantile & 6.23 $\pm$ 0.05 & 5.99 $\pm$ 0.04 & 5.50 $\pm$ 0.04 & 5.17 $\pm$ 0.03 & 4.36 $\pm$ 0.06 & 3.66 $\pm$ 0.05 & 3.12 $\pm$ 0.03 & \textbf{3.00 $\pm$ 0.00} \\ \hline \DP & 6.13 $\pm$ 0.05 & \textbf{5.73 $\pm$ 0.04} & \textbf{5.39 $\pm$ 0.04} & \textbf{5.11 $\pm$ 0.03} & \textbf{4.08 $\pm$ 0.04} & \textbf{3.63 $\pm$ 0.04} & \textbf{3.10 $\pm$ 0.03} & \textbf{3.00 $\pm$ 0.00} \\ \hline \Threshold & 6.56 $\pm$ 0.06 & 6.29 $\pm$ 0.05 & 6.14 $\pm$ 0.04 & 5.92 $\pm$ 0.03 & 4.53 $\pm$ 0.05 & 4.19 $\pm$ 0.04 & 3.92 $\pm$ 0.03 & 3.96 $\pm$ 0.03 \\ \hline \DQN & 6.67 $\pm$ 0.07 & 5.87 $\pm$ 0.05 & 5.60 $\pm$ 0.05 & 5.22 $\pm$ 0.04 & 4.51 $\pm$ 0.05 & 3.95 $\pm$ 0.05 & 3.42 $\pm$ 0.04 & 3.04 $\pm$ 0.02 \\ \hline 
\end{tabular}}

\caption{Average maximum load of \KThinning strategies with $95\%$ confidence intervals.}
\label{tab:k-thinning-comparison}
\end{table}


The key conclusions from Table~\ref{tab:k-thinning-comparison} are:


\begin{itemize}
    \item
    The \Quantile and \LocalRewardOptimiser strategies are almost as good as the \DP strategy for most values of $n$, $m$ and $k$, even though neither of those strategies consider future rewards.
    \item
    For $k>2$, the \DQN strategy is significantly better than the \Threshold strategy under a largest $p$-value of $3.21\cdot 10^{-6}$, but is worse than the \Quantile strategy under a largest $p$-value of $0.02$, and the \LocalRewardOptimiser strategy under a largest $p$-value of $1.73\cdot 10^{-4}$.
    \item
    Intuitively, the larger $k$ is, the better a strategy can do, and the table confirms it for most of the strategies. For large $k$ ($5$ or $10$) and not much larger $n$, strategies can even force the ball to a least loaded bin in most rounds.
    \item
    The advantage of a large $k$ is much easier to exploit by a manual algorithm, than to learn by \DQL.
\end{itemize}


\subsection{Analysis of Optimal Strategies}


In Figure~\ref{k-thinning-dp-maxload} we can see how the value of $k$ influences the (optimal) DP Strategy. While there is a large improvement from $k=2$ to $k=3$ (e.g. the probability of $\mathrm{maxload}=6$ becomes negligible), we can observe diminishing returns by further increasing $k$.


\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{Chapter4/Figs/k_thinning_max_load_distribution_5_20.pdf}
    \caption{Analysis of the final maximum load distribution of the DP strategy for $n=5$, $m=20$ and different values of $k$.}
    \label{k-thinning-dp-maxload}
\end{figure}

Now I present a lemma about optimal strategies:

\begin{lemma}[\KThinning monotonicity] \label{lemma: k-thinning-monotone}
There exists an optimal monotone strategy.
\end{lemma}



The proof is quite involved but it is structurally similar to the proof of Lemma~\ref{lemma: thresholdproperty}, so I leave it to Appendix~\ref{k-thinning-monotone-proof}.


\begin{remark}
Lemma~\ref{lemma: k-thinning-monotone} has been verified also by showing that this property holds for the (optimal) \DP strategy for several combinations of $n$, $m$ and $k$.
\end{remark}


\subsection{\DQL Analysis} \label{dql-analysis-k-thinning}


I leave the hyperparameter and training analysis of \DQL to Appendix~\ref{k-thinning-dql-analysis}.

\section{\GraphicalTwoChoice}


\subsection{Comparison of Strategies} \label{graphical-two-choice-comparison}


\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
                                & \multicolumn{3}{c|}{$n=4$} & \multicolumn{3}{c|}{$n=16$} & \multicolumn{3}{c|}{$n=32$}\\ \hline
                                & \multicolumn{3}{c|}{$m=25$} & \multicolumn{3}{c|}{$m=50$} & \multicolumn{3}{c|}{$m=32$}\\ \hline
Strategy                                & Cycle & Hypercube & Complete & Cycle & Hypercube & Complete & Cycle & Hypercube & Complete \\ \hline
\Greedy & 7.06 $\pm$ 0.02 & 7.06 $\pm$ 0.02 & 7.19 $\pm$ 0.04 & \textbf{4.75 $\pm$ 0.05} & \textbf{4.51 $\pm$ 0.05} & \textbf{4.40 $\pm$ 0.05} & \textbf{2.40 $\pm$ 0.05} & \textbf{2.33 $\pm$ 0.04} & \textbf{2.26 $\pm$ 0.04} \\ \hline \Random & 8.96 $\pm$ 0.12 & 8.97 $\pm$ 0.12 & 8.84 $\pm$ 0.11 & 6.49 $\pm$ 0.09 & 6.62 $\pm$ 0.10 & 6.69 $\pm$ 0.10 & 3.49 $\pm$ 0.06 & 3.52 $\pm$ 0.07 & 3.50 $\pm$ 0.06 \\ \hline \LocalRewardOptimiser & 7.12 $\pm$ 0.03 & 7.11 $\pm$ 0.03 & 7.25 $\pm$ 0.04 & 4.87 $\pm$ 0.06 & 4.77 $\pm$ 0.05 & 4.74 $\pm$ 0.05 & 2.53 $\pm$ 0.05 & 2.44 $\pm$ 0.04 & 2.38 $\pm$ 0.04 \\ \hline \DP & \textbf{7.04 $\pm$ 0.02} & \textbf{7.03 $\pm$ 0.01} & \textbf{7.17 $\pm$ 0.03} & TLE & TLE & TLE & TLE & TLE & TLE \\ \hline \DQN & 7.11 $\pm$ 0.03 & 7.11 $\pm$ 0.03 & 7.31 $\pm$ 0.05 & 4.88 $\pm$ 0.07 & 4.53 $\pm$ 0.05 & 4.50 $\pm$ 0.05 & 2.68 $\pm$ 0.06 & 2.54 $\pm$ 0.05 & 2.60 $\pm$ 0.06 \\ \hline 
\end{tabular}}

\caption{Average maximum load of \GraphicalTwoChoice strategies with $95\%$ confidence intervals\protect\footnotemark.}
\label{tab:graphical-two-choice-comparison}
\end{table}

\footnotetext{Note that for $n=4$, the \CycleGraph and the \HypercubeGraph are the same.}

The key conclusions from Table~\ref{tab:graphical-two-choice-comparison} are:

\begin{itemize}
    \item The \Greedy strategy is not exactly optimal, as can be seen from the $n=4$ case (I will provide an explicit counterexample in Lemma~\ref{lemma: greedy-suboptimal}). On the other hand, it is close to the performance of the \DP strategy, and when that is not applicable, \Greedy is by far the best.
    \item While for larger $n$, agreeing with the intuition the \CompleteGraph is the most favourable for \Greedy, it works better on the \CycleGraph for $n=4$. 
    \item The \DQN strategy performs performs consistently, but it cannot always exploit the subtle suboptimalities of \Greedy. Its performance is comparable to the performance of the \LocalRewardOptimiser strategy.
    
\end{itemize}


\subsection{Analysis of the \Greedy Strategy}

First I prove the most significant result of this section.

\begin{lemma} \label{lemma: greedy-suboptimal}
There exists a graph, such that the \Greedy strategy is suboptimal with respect to the expected final maximum load of \GraphicalTwoChoice.
\end{lemma}

\begin{proof}
Using the \DP strategy for the \CycleGraph with $n=4$ bins $m=6$ balls, I found a state $s$ (i.e.\ a load vector $v$ and edge $e$), where choosing the less loaded bin is suboptimal. Denoting the nodes as ($0$-based) indices in the load vector, the counterexample state is $v=(0,1,0,2)$,  $e=(1,2)$ i.e.\ there is an edge between the second and third bin. \Greedy would choose the third bin, but if the remaining two edges are $(2,3)$, $\mathrm{maxload}>2$ cannot be avoided (see Figure~\ref{greedy-counterexample}). On the other hand, by choosing the second bin and then picking the even-indexed endpoints of the later edges, the maximum load will always be $2$. Therefore, the expected final maximum load of choosing the second bin (and then playing optimally) is $2.0$, while that of choosing the third bin is between $2.0$ and $3.0$.
\end{proof}



\begin{figure}
    \centering
    \begin{tikzpicture}
    \Tree [.$(0,\underline{\mathbf{1}},\underline{\mathbf{0}},2)$
    [.$(0,1,\underline{\mathbf{1}},\underline{\mathbf{2}})$
    [.$(0,1,\underline{\mathbf{2}},\underline{\mathbf{2}})$
    [.$(0,1,3,2)$ ] $\ldots$ ] $\ldots$ ]
    [.$(0,2,0,2)$ $\ldots$ ]]
    \end{tikzpicture}
    \caption{Counterexample for the optimality of \Greedy, showing that choosing the third bin can lead to a maximum load of $3$ while choosing the second bin cannot.}
    \label{greedy-counterexample}
\end{figure}



To better understand the impact of the graph structure on \Greedy, I analysed the relationship between the degree $d$ of the graph and how well \Greedy performs on it. I created $1000$ random regular graphs for each degree $1\leq d \leq 31$ of the $n=m=32$ case. We can see in Figure~\ref{greedy-random-regular-analysis} that smaller degrees hurt \Greedy, but after around $d=10$, the improvement decays.


\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{Chapter4/Figs/greedy_degree_analysis_32_32.pdf}
    \caption{Relating the degree of the graph to the performance of Greedy for $n=m=32$.}
    \label{greedy-random-regular-analysis}
\end{figure}


\subsection{\DQL Analysis} \label{dql-analysis-graphical-two-choice}

To better understand the difficulty in finding an optimal strategy, I extended the \CycleGraph counterexample from Lemma~\ref{lemma: greedy-suboptimal} to load vectors of the form $(0, a, 0, b)$, with the next edge still going between the second and third bins. Figure~\ref{greedy-counterexample-analysed} and Figure~\ref{greedy-counterexample-analysed-for-dqn} show that the shape of the region containing counterexamples to the \Greedy strategy is difficult to characterise, and to precisely learn by RL (e.g.\ the boundary is non-linear between the green and red areas). By using the $\Phi_{neigh}$ potential function for \DQL, the agent is guided towards choosing second bin, and by using a graph-oblivious potential (e.g.\ $\Phi_{max}$), the agent is guided towards choosing the third bin. Therefore, neither of the potential functions is perfect. We can indeed see in Figure~\ref{greedy-counterexample-analysed-for-dqn} that the DQN could not learn the exact pattern, though there is some resemblance.


\begin{figure}
\centering
\begin{minipage}[t]{.48\linewidth}
  \centering
  \includegraphics[scale=0.45]{Chapter4/Figs/0a0b_4_25_analysis.png}
  \caption{The optimal decisions for the \CycleGraph with $n=4$, $m=25$, load vector $(0,a,0,b)$ and edge $(2,3)$. Green indicates choosing the second bin is better, red indicates choosing the third bin is better, and $x$ indicates that they have the same expected score.}
  \label{greedy-counterexample-analysed}
\end{minipage}\hfill
\begin{minipage}[t]{.48\linewidth}
  \centering
  \includegraphics[scale=0.45]{Chapter4/Figs/0a0b_4_25_analysis_dqn.png}
  \caption{The decisions made by the \DQN strategy in the situation from Figure~\ref{greedy-counterexample-analysed}.}
  \label{greedy-counterexample-analysed-for-dqn}
\end{minipage}
\end{figure}



\subsubsection*{Hyperparameter Analysis}


Analysing Table~\ref{graphical-two-choice-hyperparameter-importance}, the negative correlation of ``hidden\_size'' and ``num\_lin\_layers'' with the score suggests that adding more parameters to the DQN does not help. 


\begin{table}[h]
\begin{center}
\begin{tabular}{lcc}
 \textbf{Hyperparameter} & \textbf{Importance} & \textbf{Correlation} \\
 \addlinespace[0.2cm]
 \texttt{pre\_train\_episodes} & \Progress{0.207}{blue} & \Progress{0.450}{green} \\
 \texttt{num\_lin\_layers} & \Progress{0.190}{blue} & \Progress{0.344}{red} \\
 \texttt{hidden\_size} & \Progress{0.162}{blue} & \Progress{0.462}{red} \\
 \texttt{optimise\_freq} & \Progress{0.074}{blue} & \Progress{0.333}{red} \\
 \texttt{target\_update\_freq} & \Progress{0.071}{blue} & \Progress{0.124}{red} \\
\end{tabular}
\caption{\GraphicalTwoChoice hyperparameter importance for the \CycleGraph with $n=4$ and $m=25$ \cite{biewald2020wandb}.}
\label{graphical-two-choice-hyperparameter-importance}
\end{center}
\end{table}


\subsubsection*{Training}

Figure~\ref{graphical-two-choice-training-curve} shows a much steadier improvement during training for \GraphicalTwoChoice than what we saw for \TwoThinning and \KThinning. This is mostly because there is no reasonable easy-to-learn initial strategy for \GraphicalTwoChoice with the chosen MDP formulation, unlike the \ConstantOffset strategies for \TwoThinning.

\begin{figure}[h] 
    \centering
    \includegraphics[scale=0.6]{Chapter4/Figs/training_progression_hypercube_32_32.pdf}
    \caption{\GraphicalTwoChoice training curve for the \CycleGraph with $n=m=32$.}
    \label{graphical-two-choice-training-curve}
\end{figure}

