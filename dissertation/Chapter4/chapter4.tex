%!TEX root = ../thesis.tex
\chapter{Evaluation}\label{evaluation}


\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi


\NOTE{A}{I am really not sure about the best structure for this chapter.}

\NOTE{A}{Add theoretical results. Maybe implementation?}

\section{General Notes}


\NOTE{A}{Note that in this project there is no train or test set, there isnâ€™t any data at all. Nevertheless, it is still important to do evaluation independently of early stopping, due to the Expectation-Maximum Jensen inequality.}


\section{Two-Thinning}


\subsection{Comparison of Strategies}



\NOTE{A}{Mention that local reward optimiser is just away from max. Add plot showing that it doesn't work very well for larger $n$ and $m$. Add general plots for huge $n$ and $m$.}
\NOTE{A}{Explain why DP is run experimentally even though there is an exact value for its expected value - the extreme values are very rare which make the comparison less fair for 100 runs}
\NOTE{A}{State the optimal threshold of the threshold strategy}

\NOTE{A}{Add theoretical bounds}

\NOTE{A}{Mention that it might make sense to repeat not just the runs but also the training several times. Depends on what we want to evaluate exactly? Note that the number of hyperparameter tuning iterations can also be considered as training epochs, since hyperparamters and neural network parameters are both actually paramters}

\NOTE{A}{Mention that confidence interval is based on CLT.}

\begin{table}[h!]
\caption{Average maximum load of \textsc{Two-Thinning} strategies with $95\%$ confidence intervals}
\label{tab:two-thinning-comparison}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
                                & \multicolumn{3}{c|}{$n=5$} & \multicolumn{3}{c|}{$n=20$} & \multicolumn{3}{c|}{$n=50$} \\ \hline
                                & $m=5$ & $m=10$ & $m=15$ & $m=20$ & $m=60$ & $m=400$ & $m=50$ & $m=200$ & $m=2500$ \\ \hline
Always Accept Strategy          & 2.26 $\pm$ 0.06 & 3.84 $\pm$ 0.1 & 7.88 $\pm$ 0.14 & 3.26 $\pm$ 0.1 & 6.68 $\pm$ 0.15 & 28.84 $\pm$ 0.32 & 3.87 $\pm$ 0.1 & 9.12 $\pm$ 0.16 & 66.66 $\pm$ 0.5 \\ \hline
Random Strategy                 & 2.35 $\pm$ 0.07 & 3.72 $\pm$ 0.09 & 7.66 $\pm$ 0.14 & 3.21 $\pm$ 0.1 & 6.67 $\pm$ 0.15 & 28.55 $\pm$ 0.32 & 3.8 $\pm$ 0.1 & 9.02 $\pm$ 0.18 & 66.83 $\pm$ 0.48 \\ \hline
Local Reward Optimiser Strategy & 1.82 $\pm$ 0.05 & 2.97 $\pm$ 0.05 & 6.08 $\pm$ 0.06 & 2.25 $\pm$ 0.06 & 4.75 $\pm$ 0.08 & 22.46 $\pm$ 0.09 & 2.54 $\pm$ 0.07 & 6.37 $\pm$ 0.07 & 53.98 $\pm$ 0.11 \\ \hline
Mean Thinning Strategy          & 1.87 $\pm$ 0.07 & 3.1 $\pm$ 0.07 & 6.17 $\pm$ 0.09 & 2.56 $\pm$ 0.08 & 5.12 $\pm$ 0.1 & 22.52 $\pm$ 0.12 & 3.06 $\pm$ 0.08 & 6.79 $\pm$ 0.11 & 53.53 $\pm$ 0.15 \\ \hline
DP Strategy                     & 1.85 $\pm$ 0.06 & 2.98 $\pm$ 0.07 & 6.06 $\pm$ 0.1 & 2.33 $\pm$ 0.1 & 4.69 $\pm$ 0.11 & TLE & 2.52 $\pm$ 0.1 & TLE & TLE \\ \hline
Deep Q-Learning Strategy        & 1.92 $\pm$ 0.09 & 2.93 $\pm$ 0.09 & 6.2 $\pm$ 0.11 & 2.42 $\pm$ 0.1 & 4.88 $\pm$ 0.12 & 22.2 $\pm$ 0.14 & 2.5 $\pm$ 0.11 & 6.43 $\pm$ 0.11 & 53.49 $\pm$ 0.25 \\ \hline
Threshold Strategy              & 2.18 $\pm$ 0.13 & 3.32 $\pm$ 0.13 & 6.44 $\pm$ 0.13 & 2.58 $\pm$ 0.12 & 5.58 $\pm$ 0.17 & 24.12 $\pm$ 0.19 & 2.99 $\pm$ 0.12 & 7.02 $\pm$ 0.13 & 56.73 $\pm$ 0.29 \\ \hline 

\end{tabular}}
\end{table}


\NOTE{A}{Do P-value tests.}

\NOTE{A}{Try to explain why it is so hard for DQN to learn good policies.}

\NOTE{A}{Reason about why I use X runs for evaluation. Is there any formal reasoning related to statistical significance? I couldn't find any.}
\NOTE{A}{Should the ``evaluation'' of how the score progression during training be presented in the Implementation or the Evaluation chapter? Add training curve somewhere.}


\subsection{Deep Q-Learning analysis}


\begin{lemma}\label{lemma: two-thinning-increasing-threshold}
The chosen thresholds of an optimal strategy are non-decreasing during an execution. That is, if for $i$th ball the strategy chooses a threshold $x$, then for every $j$th ball of the same game such that $j>i$, a threshold $y\geq x$ is chosen.
\end{lemma}


\NOTE{A}{Add figure of increasing threshold of DP strategy.}


\begin{proof}

\NOTE{A}{Do proper proof.}

The lemma has been verified by the optimal strategy(s) generated by dynamic programming, for several combinations of $n$ and $m$. 
\end{proof}


\subsubsection{Training}


\subsubsection{Hyperparameter Importance}



\section{K-Thinning}



\subsection{Comparison of Strategies}



\begin{table}[h!]
\caption{Average maximum load of \textsc{K-Thinning} strategies with $95\%$ confidence intervals}
\label{tab:k-thinning-comparison}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
                                & \multicolumn{4}{c|}{$n=5$} & \multicolumn{4}{c|}{$n=25$}\\ \hline
                                & \multicolumn{4}{c|}{$m=20$} & \multicolumn{4}{c|}{$m=50$}\\ \hline
                                & $k=2$ & $k=3$ & $k=5$ & $k=10$ & $k=2$ & $k=3$ & $k=5$ & $k=10$ \\ \hline
Always Accept Strategy          & 7.88 $\pm$ 0.14 & 7.72 $\pm$ 0.11 & 7.81 $\pm$ 0.14 & 7.8 $\pm$ 0.14 & 5.65 $\pm$ 0.19 & 5.81 $\pm$ 0.13 & 5.84 $\pm$ 0.14 & 6.03 $\pm$ 0.16 \\ \hline
Random Strategy                 & 7.66 $\pm$ 0.14 & 7.76 $\pm$ 0.13 & 7.77 $\pm$ 0.14 & 7.74 $\pm$ 0.14 & 5.77 $\pm$ 0.19 & 5.96 $\pm$ 0.15 & 5.87 $\pm$ 0.15 & 5.72 $\pm$ 0.14 \\ \hline
Local Reward Optimiser Strategy & 6.08 $\pm$ 0.06 & 5.77 $\pm$ 0.05 & 5.45 $\pm$ 0.06 & 5.13 $\pm$ 0.04 & 4.22 $\pm$ 0.11 & 3.69 $\pm$ 0.06 & 3.21 $\pm$ 0.06 & 3.0 $\pm$ 0.01 \\ \hline
Mean Thinning Strategy          & 6.17 $\pm$ 0.09 & 5.96 $\pm$ 0.04 & 5.61 $\pm$ 0.06 & 5.15 $\pm$ 0.04 & 4.34 $\pm$ 0.12 & 3.69 $\pm$ 0.07 & 3.15 $\pm$ 0.05 & 3.0 $\pm$ 0.01 \\ \hline
DP Strategy & 6.06 $\pm$ 0.1   & 5.75 $\pm$ 0.05 & 5.38 $\pm$ 0.05 & 5.1 $\pm$ 0.03 & 4.13 $\pm$ 0.09 & 3.58 $\pm$ 0.07 & 3.15 $\pm$ 0.05 & 3.0 $\pm$ 0.0 \\ \hline Threshold Strategy              & 6.44 $\pm$ 0.13 & 6.27 $\pm$ 0.05 & 6.12 $\pm$ 0.05 & 5.93 $\pm$ 0.05 & 4.72 $\pm$ 0.15 & 4.17 $\pm$ 0.05 & 4.0 $\pm$ 0.05 & 3.94 $\pm$ 0.05 \\ \hline
Deep Q-Learning Strategy        & 6.2 $\pm$ 0.11 & 6.77 $\pm$ 0.11 & 7.14 $\pm$ 0.14 & 7.14 $\pm$ 0.14 & 4.26 $\pm$ 0.09 & 4.47 $\pm$ 0.09 & 4.21 $\pm$ 0.08 & 3.01 $\pm$ 0.01 \\ \hline 

\end{tabular}}
\end{table}


\begin{lemma} \label{lemma: k-thinning-increasing-threshold}
In an optimal strategy, it is not possible that it would accept a bin with $x$ choices left, but reject the same bin (with the same loads) with $y<x$ choices left. In other words, an optimal strategy should never become more selective after rejecting some options.
\end{lemma}


\begin{proof}
Note that Lemma \ref{lemma: thresholdproperty} generalises to \textsc{K-Thinning} as well - an optimal strategy must base its decisions on a threshold.


\NOTE{A}{TODO: add real proof}.


Using the dynamic programming algorithm, that creates the optimal strategy(s), I verified the lemma, that is that the threshold is indeed non-decreasing within the same load configuration.
\end{proof}


\subsection{Deep Q-Learning analysis}


\subsubsection{Training}


\subsubsection{Hyperparameter Importance}





\section{Graphical Two-Choice}


\subsection{Comparison of Strategies}


\begin{table}[h!]
\caption{Average maximum load of \textsc{Graphical Two Choice} strategies with $95\%$ confidence intervals}
\label{tab:graphical-two-choice-comparison}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
                                & \multicolumn{3}{c|}{$n=4$} & \multicolumn{3}{c|}{$n=16$} & \multicolumn{3}{c|}{$n=32$}\\ \hline
                                & \multicolumn{3}{c|}{$m=25$} & \multicolumn{3}{c|}{$m=50$} & \multicolumn{3}{c|}{$m=32$}\\ \hline
                                & Cycle & Clique & Hypercube & Cycle & Clique & Hypercube & Cycle & Clique & Hypercube \\ \hline
Greedy Strategy & 7.04 $\pm$ 0.03 & 7.04 $\pm$ 0.03 & 7.17 $\pm$ 0.06 & 4.74 $\pm$ 0.08 & 4.47 $\pm$ 0.11 & 4.39 $\pm$ 0.07 & 2.46 $\pm$ 0.1 & 2.34 $\pm$ 0.09 & 2.26 $\pm$ 0.06 \\ \hline
Random Strategy  & 8.92 $\pm$ 0.17 & 8.81 $\pm$ 0.18 & 8.98 $\pm$ 0.17 & 6.75 $\pm$ 0.17 & 6.64 $\pm$ 0.23 & 6.56 $\pm$ 0.15 & 3.45 $\pm$ 0.13 & 3.64 $\pm$ 0.16 & 3.54 $\pm$ 0.1 \\ \hline
Local Reward Optimiser Strategy & 7.1 $\pm$ 0.04 & 7.12 $\pm$ 0.05 & 7.29 $\pm$ 0.07 & 4.97 $\pm$ 0.1 & 4.74 $\pm$ 0.12 & 4.75 $\pm$ 0.07 & 2.53 $\pm$ 0.11 & 2.42 $\pm$ 0.1 & 2.46 $\pm$ 0.07 \\ \hline
DP Strategy & 7.01 $\pm$ 0.02 & 7.01 $\pm$ 0.02 & 7.27 $\pm$ 0.1 & TLE & TLE & TLE & TLE & TLE & TLE \\ \hline
Deep Q-Learning Strategy & 7.17 $\pm$ 0.07 & 7.17 $\pm$ 0.06 & 7.38 $\pm$ 0.09 & 4.92 $\pm$ 0.15 & 5.71 $\pm$ 0.17 & 6.57 $\pm$ 0.15 & 3.81 $\pm$ 0.18 & 3.59 $\pm$ 0.18 & 4.17 $\pm$ 0.14 \\ \hline 

\end{tabular}}
\end{table}


\NOTE{A}{Does connectedness matter for small graphs? Add analysis for random regular graphs.}


\begin{lemma} \label{lemma: greedy-suboptimal}
There exist a graph, such that the Greedy strategy is suboptimal with respect to the expected final maximum load of \textsc{Graphical Two Choice}.
\end{lemma}

\begin{proof}
It is enough to show that there is a state $s$ (i.e. a load vector $v$, edge $e$) where is is better to choose the bin with the larger load. This suffices because there is a non-zero probability of reaching $s$, and hence a strategy which agree with Greedy everywhere else except for this state has a strictly better expected score.


I used the dynamic programming algorithm to find the optimal strategy for \textsc{Graphical Two Choice}, and then I searched the strategy for a state where it disagrees with Greedy. I found a very small counterexample for the Cycle graph with $n=4$ bins $m=6$ balls. Denoting the nodes as ($1$-based) indices in the load vector, the counterexample state is $l=(2,0,1,0)$, $e=(2,3)$, i.e. there is an edge between the second and third bin. The reason why it is better to choose bin $3$ even though it has larger load than bin $2$ is that after that, from load vector $(2,0,2,0)$ with $2$ balls remaining we can definitely avoid having a maximum load $3$ by always placing the ball in an even-indexed bin. On the other hand, from load vector $(2,1,1,0)$, if both of the remaining edges are $(1,2)$, then there will be a maximum load of at least $3$ using whatever strategy. Therefore, the expected final maximum load of choosing bin $3$ is $2.0$, while that of bin $2$ is between $2.0$ and $3.0$.
\end{proof}

\subsection{Deep Q-Learning analysis}


\subsubsection{Training}


\subsubsection{Hyperparameter Importance}



\section{Behavioural Analysis of Two-Thinning (\textit{Extension})}


\subsection{Threshold ``Spikes'' During Training}


\subsection{Explaining Trained Models}